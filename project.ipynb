{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs, collections, csv, pymorphy2, re\n",
    "\n",
    "from time import gmtime, strftime, time\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.spatial import distance\n",
    "from sklearn import linear_model, metrics, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lib:\n",
    "    def process_titles():\n",
    "        data = {}\n",
    "        with open('docs_titles.tsv') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                content = line.strip().split('\\t', 1)\n",
    "                doc_id = int(content[0])\n",
    "                if len(content) == 1:\n",
    "                    title = ''\n",
    "                else:\n",
    "                    title = content[1]\n",
    "                data[doc_id] = title\n",
    "        return pd.Series(data)\n",
    "    \n",
    "    train_data = pd.read_csv('train_groups.csv')\n",
    "    test_data = pd.read_csv('test_groups.csv')\n",
    "    titles = process_titles()\n",
    "    g_arange = {\n",
    "        'train': np.arange(1, 130),\n",
    "        'test': np.arange(130, 310),\n",
    "    }\n",
    "    \n",
    "    log_path = 'data/'\n",
    "    features_path = lambda group_id: 'data/f_{}.npy'.format(group_id)\n",
    "    target_path = lambda group_id: 'data/t_{}.npy'.format(group_id)\n",
    "    lxml_str = lambda lxml: ''.join(map(lambda x: ' ' + x.text, lxml.find_all(re.compile('^h[1-6]$'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class consts:\n",
    "    max_features = 25\n",
    "    title_weight = 5\n",
    "    \n",
    "    function_words = {'INTJ', 'PRCL', 'CONJ', 'PREP'}\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    separators = [';', ':', '-', '_', '|', ',',\n",
    "                  '.', '!', '?', '«', '»', '\"',\n",
    "                  '(', ')', '[', ']', '–', '”',\n",
    "                  '*', '�', '^', '=', '©', '“',\n",
    "                  '’', '+', '…', '&', '—', '$',\n",
    "                  '/', '→', '←', '{', '}', '`',\n",
    "                  '°', '@', '#',\n",
    "                  '\\n', '\\t', '\\r', '\\'', '\\\\']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text, separators = consts.separators):\n",
    "    text = text.lower()\n",
    "    for sep in separators:\n",
    "        text = text.replace(sep, ' ')\n",
    "    return text.split()\n",
    "\n",
    "def normalizer(text):\n",
    "    text = split(text)\n",
    "    text = list(filter(lambda x: consts.morph.parse(x)[0].tag.POS not in consts.function_words, text))\n",
    "    text = map(lambda x: consts.morph.parse(x)[0].normal_form, text)\n",
    "    text = map(lambda x: x.replace('ё', 'е'), text)\n",
    "    text = list(filter(lambda x: not re.search('\\d+', x) and len(x) > 1, text))\n",
    "    return ' '.join(text)\n",
    "\n",
    "def doc_handler(doc_id):\n",
    "    path = 'content/{}.dat'.format(doc_id)\n",
    "    with codecs.open(path, 'r', 'utf-8') as f:\n",
    "        url = f.readline().strip()\n",
    "        text = normalizer(lib.lxml_str(BeautifulSoup(f, 'lxml')) + consts.title_weight * (' ' + lib.titles[doc_id]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(func):\n",
    "    def wrapper(mode):\n",
    "        timer = time()\n",
    "        result = func(mode)\n",
    "        with open(lib.log_path + 'report.logs', 'w') as f:\n",
    "            f.write('mode: {}\\n'.format(mode))\n",
    "            f.write('total time: {}\\n'.format(strftime('%H:%M:%S', gmtime(np.round(time() - timer)))))\n",
    "            f.write('max_features = {}\\n'.format(consts.max_features))\n",
    "            f.write('title_weight = {}\\n'.format(consts.title_weight))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def export_data(mode, group_id):\n",
    "    if mode == 'train':\n",
    "        docs = [doc_handler(doc) for doc in \n",
    "                         lib.train_data[lib.train_data.group_id == group_id].doc_id]\n",
    "        data = TfidfVectorizer().fit_transform(docs).todense()\n",
    "        \n",
    "    elif mode == 'test':\n",
    "        docs = [doc_handler(doc) for doc in \n",
    "                         lib.test_data[lib.test_data.group_id == group_id].doc_id]\n",
    "        data = TfidfVectorizer().fit_transform(docs).todense()\n",
    "        \n",
    "    dist = distance.pdist(data, metric = 'cosine')\n",
    "    dist[np.where(np.isnan(dist))[0]] = 1.0\n",
    "    dist = distance.squareform(dist)\n",
    "    \n",
    "    np.save(lib.features_path(group_id), np.sort(dist)[:, 1:1 + consts.max_features:])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        targets = np.array(lib.train_data[lib.train_data.group_id == group_id].target)\n",
    "        np.save(lib.target_path(group_id), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logger\n",
    "def process_data(mode):\n",
    "    if mode == 'train':\n",
    "        arange = lib.g_arange['train']\n",
    "        for group_id in arange:\n",
    "            export_data(mode, group_id)\n",
    "            \n",
    "    elif mode == 'test':\n",
    "        arange = lib.g_arange['test']\n",
    "        for group_id in arange:\n",
    "            export_data(mode, group_id)\n",
    "            \n",
    "    elif mode == 'all':\n",
    "        process_data('train')\n",
    "        process_data('test')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('mode matches neither \\'train\\' nor \\'test\\' nor \\'all\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logger\n",
    "def collect_data(mode):\n",
    "    if mode == 'train':\n",
    "        arange = lib.g_arange['train']\n",
    "        targets = np.load(lib.target_path(arange[0]))\n",
    "        features = np.load(lib.features_path(arange[0]))\n",
    "        \n",
    "        for group_id in arange[1::]:\n",
    "            group_targets = np.load(lib.target_path(group_id))\n",
    "            group_features = np.load(lib.features_path(group_id))\n",
    "\n",
    "            targets = np.concatenate((targets, group_targets))\n",
    "            features = np.concatenate((features, group_features))\n",
    "            \n",
    "        return features, targets\n",
    "    \n",
    "    elif mode == 'test':\n",
    "        arange = lib.g_arange['test']\n",
    "        features = np.load(lib.features_path(arange[0]))\n",
    "        \n",
    "        for group_id in arange[1::]:\n",
    "            group_features = np.load(lib.features_path(group_id))\n",
    "            features = np.concatenate((features, group_features))\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    elif mode == 'all':\n",
    "        train_data = collect_data('train')\n",
    "        test_data = collect_data('test')\n",
    "        return train_data[0], train_data[1], test_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('mode matches neither \\'train\\' nor \\'test\\' nor \\'all\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 7s, sys: 21.6 s, total: 27min 29s\n",
      "Wall time: 28min 24s\n",
      "cat: /data/report.logs: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%time process_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test\n",
      "total time: 00:28:25\n",
      "max_features = 25\n",
      "title_weight = 5CPU times: user 432 ms, sys: 314 ms, total: 746 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "!cat data/report.logs\n",
    "%time X_train, y_train, X_test = collect_data('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "data_train, data_test, label_train, label_test = model_selection.train_test_split(X_train,\n",
    "                                                                                  y_train,\n",
    "                                                                                  test_size=0.1,\n",
    "                                                                                  stratify=y_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263157894736841"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.SGDClassifier(loss='log',\n",
    "                                   penalty='l1',\n",
    "                                   alpha=0.00002,\n",
    "                                   l1_ratio=0.01,\n",
    "                                   max_iter=10000,\n",
    "                                   n_jobs=-1,\n",
    "                                   n_iter_no_change=1,\n",
    "                                   class_weight='balanced')\n",
    "model.fit(data_train, label_train)\n",
    "metrics.f1_score(label_test, model.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "pred = model.predict(X_test)\n",
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['pair_id', 'target'])\n",
    "    writer.writeheader()\n",
    "    i = 11691\n",
    "    for elem in pred:\n",
    "        writer.writerow({'pair_id': str(i), 'target': str(elem)})\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
